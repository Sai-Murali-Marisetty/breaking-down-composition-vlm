# Breaking Down Composition: Analyzing Failure Modes in Vision-Language Models

A systematic evaluation of compositional reasoning capabilities in Vision-Language Models (VLMs) and the effectiveness of prompt-based interventions.

## ğŸ“‹ Project Overview

### Research Questions

1. What specific compositional reasoning capabilities do modern VLMs lack?
2. Can simple prompting strategies improve their performance without retraining?

### Key Findings

- âœ… **CLIP baseline**: 31.25% group score on Winoground (matches published benchmarks)
- âœ… **Error taxonomy**: Identified 5 major failure categories
- âœ… **Prompting effectiveness**: Zero improvement for CLIP (embedding model)
- ğŸ”„ **In progress**: Testing generative models (LLaVA, SmolVLM)

---

## ğŸ—‚ï¸ Repository Structure

```
breaking-down-composition-vlm/
â”œâ”€â”€ data/                          # Datasets (not included in git)
â”‚   â”œâ”€â”€ winoground/               # 400 examples
â”‚   â””â”€â”€ aro/                      # 52K examples
â”‚
â”œâ”€â”€ models/                        # Model infrastructure
â”‚   â”œâ”€â”€ model_loader.py           # Load CLIP, LLaVA, SmolVLM
â”‚   â””â”€â”€ inference_engine.py       # Inference wrapper
â”‚
â”œâ”€â”€ scripts/                       # Evaluation scripts
â”‚   â”œâ”€â”€ baseline_winoground_clip.py
â”‚   â””â”€â”€ baseline_aro_clip.py
â”‚
â”œâ”€â”€ results/                       # Evaluation results
â”‚   â”œâ”€â”€ *.json                    # Summary metrics
â”‚   â””â”€â”€ *.csv                     # Detailed results
â”‚
â”œâ”€â”€ Dataset loaders
â”œâ”€â”€ load_winoground_local.py      # Winoground dataset loader
â”œâ”€â”€ load_aro_updated.py           # ARO dataset loader
â”œâ”€â”€ verify_aro_data.py            # Data verification
â”‚
â”œâ”€â”€ Evaluation scripts
â”œâ”€â”€ baseline_simple.py            # Simple baseline evaluation
â”œâ”€â”€ evaluate_prompts.py           # Prompting strategies evaluation
â”œâ”€â”€ evaluate_llava_baseline.py    # LLaVA evaluation
â”‚
â”œâ”€â”€ Analysis
â”œâ”€â”€ analyze_results.py            # Results analysis
â”œâ”€â”€ prompting_strategies.py       # Prompt templates
â”‚
â”œâ”€â”€ Testing
â”œâ”€â”€ simple_test.py                # Quick model test
â”œâ”€â”€ test_llava_setup.py           # LLaVA setup verification
â”œâ”€â”€ test_models_setup.py          # Full model test
â”œâ”€â”€ test_with_winoground.py       # Winoground test
â”‚
â”œâ”€â”€ Configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Git ignore rules
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone <your-repo-url>
cd breaking-down-composition-vlm

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download Datasets

**Winoground** (400 examples):
- Visit: https://huggingface.co/datasets/facebook/winoground
- Accept terms and download to `data/winoground/`

**ARO** (52K examples):
- Clone: https://github.com/mertyg/vision-language-models-are-bows
- Copy data files to `data/aro/`

### 3. Run Baseline Evaluation

```bash
# Quick test (5 examples)
python test_with_winoground.py

# Full evaluation
python baseline_simple.py
```

---

## ğŸ“Š Results Summary

### CLIP ViT-B/32 Performance

**Winoground (400 examples)**:
- **Group Score**: 31.25%
- **Text Score**: 64.50%
- **Image Score**: 31.25%

**ARO (2,000 examples)**:
- **VG-Relation**: 54.40%
- **VG-Attribution**: 56.90%
- **Overall**: 55.65%

### Prompting Strategy Results (CLIP)

| Strategy | Group Score | Change |
|----------|-------------|--------|
| Zero-Shot (Baseline) | 31.25% | - |
| Explicit Decomposition | 31.25% | 0.00% |
| Chain-of-Thought | 31.25% | 0.00% |
| Contrastive | 31.25% | 0.00% |

**Finding**: Prompting has **zero effect** on embedding-based models like CLIP.

### Error Taxonomy

1. **Attribute Confusion** (High severity)
   - Cannot distinguish colors, sizes, ages
   
2. **Relation/Role Confusion** (High severity)
   - Fails to identify agent vs patient roles
   
3. **Word Order Insensitivity** (Medium severity)
   - Ignores word sequence changes
   
4. **Negation Failures** (High severity)
   - Cannot process negations correctly
   
5. **Spatial Reasoning** (Medium severity)
   - Struggles with spatial prepositions

---

## ğŸ”¬ Methodology

### Models Evaluated

1. **CLIP ViT-B/32** (151M params) âœ… Complete
   - Embedding-based model
   - Zero-shot baseline: 31.25%
   
2. **LLaVA-1.5-7B** (7B params) ğŸ”„ In Progress
   - Generative model
   - Expected to benefit from prompting
   
3. **SmolVLM-Instruct** (2B params) â³ Planned
   - Small efficient model

### Evaluation Datasets

1. **Winoground** (400 examples)
   - Minimal pairs with identical words, different order
   - Tests: text score, image score, group score
   
2. **ARO** (52K examples)
   - VG-Relation: Tests relational understanding
   - VG-Attribution: Tests attribute recognition

### Prompting Strategies

1. **Zero-Shot Baseline**: Standard caption
2. **Explicit Decomposition**: Break into objects â†’ attributes â†’ relationships
3. **Chain-of-Thought**: Step-by-step reasoning
4. **Contrastive**: Explicit comparison between options

---

## ğŸ“ˆ Key Insights

### 1. CLIP's Limitations

- **Strong text encoder** (64.5% text score)
- **Weak image-text alignment** (31.25% image score)
- **Bottleneck**: Image understanding, not caption processing

### 2. Prompting Ineffectiveness (CLIP)

- Zero improvement across all strategies
- Embedding models cannot process reasoning instructions
- Architecture fundamentally limits compositional reasoning

### 3. Error Patterns

- Worst performance on negation and relations
- Better on simple attributes and colors
- Significant variation by compositional tag type

---

## ğŸ› ï¸ Technical Details

### System Requirements

- **RAM**: 24GB+ recommended
- **GPU**: CUDA-capable (25GB+ for LLaVA)
- **Python**: 3.8+

### Key Dependencies

- PyTorch 2.0+
- Transformers 4.35+
- CLIP (OpenAI)
- Pillow, pandas, tqdm

### Memory Usage

| Model | Memory (FP16) | Status |
|-------|---------------|--------|
| CLIP | ~0.6 GB | âœ… Tested |
| LLaVA | ~14 GB | ğŸ”„ Testing |
| SmolVLM | ~4 GB | â³ Planned |

---

## ğŸ“ Usage Examples

### Evaluate CLIP on Winoground

```python
from models.model_loader import ModelLoader
from load_winoground_local import WinogroundLocalLoader

# Load dataset
loader = WinogroundLocalLoader()
loader.load()

# Load model
model_loader = ModelLoader()
model_loader.load_clip()

# Run evaluation
python baseline_simple.py
```

### Test Prompting Strategies

```python
python evaluate_prompts.py
# Choose: 1 (Quick test) or 2 (Full evaluation)
```

### Analyze Results

```python
python analyze_results.py
# Generates tag analysis and error taxonomy
```

---

## ğŸ“Š Results Files

All results saved to `results/` directory:

- `*_summary_*.json`: Overall metrics
- `*_detailed_*.csv`: Per-example results  
- `*_by_tag_*.csv`: Analysis by compositional tag
- `error_taxonomy_draft.json`: Failure category taxonomy

---

## ğŸ“ Academic Context

### Related Work

- **Winoground** (Thrush et al., 2022): Benchmark for visio-linguistic compositionality
- **ARO** (Yuksekgonul et al., 2023): Vision-language models as bags-of-words
- **CLIP** (Radford et al., 2021): Contrastive language-image pre-training
- **LLaVA** (Liu et al., 2023): Visual instruction tuning

### Contributions

1. Systematic evaluation across models and strategies
2. Comprehensive error taxonomy (5 categories)
3. Quantitative evidence of prompting ineffectiveness for embedding models
4. Cross-architecture comparison (embedding vs generative)

---

## ğŸ“… Project Timeline

- **Weeks 1-2**: Data preparation âœ…
- **Week 3**: Baseline evaluation âœ…
- **Week 4**: Prompting strategies âœ… (CLIP complete)
- **Week 5**: Midterm report ğŸ”„
- **Weeks 6-8**: Full experiments (LLaVA, SmolVLM)
- **Weeks 9-10**: Final paper

---

**Last Updated**: October 26, 2025  
**Status**: Week 4 - Baseline evaluation complete, LLaVA testing in progress